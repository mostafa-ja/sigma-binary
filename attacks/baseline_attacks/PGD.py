from utils.attack_utils import forward_model, update_bounds_and_consts
from attacks.binary_rounding_methods.thresholded_binary_rounding import thresholded_binary_rounding
import torch
from torch import Tensor, nn


def PGD(
    model: nn.Module,
    original_inputs: Tensor,
    feature_mask: Tensor,
    max_iterations: int = 1000,
    step_length: float = 1.0,
    norm: str = 'L1',
    verbose: bool = False,
    detector_enabled: bool = False,
    oblivion: bool = False,
    binary_search_steps: int = 4,
    initial_const = 1.0,
    device: torch.device = torch.device('cpu')
) -> Tensor:

    """
    Performs a Projected Gradient Descent (PGD) attack on a given model to
    generate adversarial examples based on binary input features.

    Args:
        model (nn.Module):
            The target neural network model to attack. It should output logits (and optionally,
            detector probabilities if `detector_enabled` is True).

        original_inputs (Tensor):
            Input samples to be perturbed. Shape: [batch_size, num_features].
            Must contain binary values (0 or 1).

        feature_mask (Tensor):
            Boolean mask indicating which features are fixed (non-modifiable) or can be perturbed.
            Shape: [num_features]. A value of 1 means the feature can be changed.

        max_iterations (int, optional, default=5000):
            Maximum number of inner optimization steps for generating adversarial perturbations.

        step_length (float, optional, default=0.01):
            Step size used for updating perturbations at each iteration (analogous to learning rate).

        verbose (bool, optional, default=True):
            If True, prints detailed progress information for each iteration, including loss,
            success rate, and distance metrics.

        detector_enabled (bool, optional, default=False):
            If True, assumes the model has an auxiliary detector output for adversarial detection.
            Used to compute additional loss components and binary search refinement.

        oblivion (bool, optional, default=False):
            If True, disables detector-related constraints, effectively ignoring the detector module
            even if present.

        binary_search_steps (int, optional, default=6):
            Number of binary search steps used to refine the attackâ€™s balancing constant (CONST)
            when `detector_enabled` is True and `oblivion` is False.

        initial_const (float, optional, default=1.0):
            Initial balancing constant for the adversarial loss when the detector is active.
            Controls the trade-off between attack strength and detection avoidance.

        device (torch.device, optional, default=torch.device('cpu')):
            Computation device to use (CPU or CUDA). If CUDA, memory usage is tracked and reported.

        cuda_warmup_iters (int, optional, default=5):
            Number of dummy CUDA operations performed before attack execution to stabilize
            GPU memory allocation. Set to 0 to disable warmup.

    Returns:
        Tensor:
            A tensor of the same shape as `original_inputs`, representing adversarial binary samples
            generated by the attack.

    Notes:
        - The function performs iterative gradient-based perturbations to flip input bits and
          induce misclassification.
        - When a detector is active, it incorporates a combined loss balancing misclassification
          and detector evasion objectives.
    """

    # loss
    criterion = nn.CrossEntropyLoss(reduction='none')

    # Initialization
    model.eval()
    original_inputs = original_inputs.to(device)
    feature_mask = feature_mask.to(device)
    batch_size, num_features = original_inputs.shape

    targets = torch.ones(batch_size, dtype=torch.long, device=device)

    # Create mask for non-fixed features
    modifiable_features_mask = torch.bitwise_or(
        feature_mask.expand(batch_size, -1), 1 - original_inputs.to(torch.uint8)
    )
    
    # ---- MODIFICATION START: Check for initially successful samples ----
    with torch.no_grad():
        initial_logits, initial_prob = forward_model(original_inputs, model, detector_enabled, oblivion)
        # Determine the detector threshold (tau) if applicable
        is_active = detector_enabled and not oblivion
        tau = model.tau[0].item() if is_active else 0.0
        
        # Define the success condition
        initially_successful_mask = (initial_logits.argmax(dim=1) == 0)
        if is_active:
            initially_successful_mask &= (initial_prob <= tau)

    # Initialize tracking variables
    best_delta = torch.zeros_like(original_inputs, device=device)
    o_success_samples = torch.zeros(batch_size, dtype=torch.bool, device=device)

    # Initialize CONST and bounds
    CONST = torch.full((batch_size,), initial_const, dtype=torch.float32, device=device)
    lower_bound = torch.zeros(batch_size, dtype=torch.float32, device=device)
    upper_bound = torch.full((batch_size,), 1e8, dtype=torch.float32, device=device)

    # Outer loop for binary search
    outer_iters = binary_search_steps if (detector_enabled and not oblivion) else 1
    for outer_step in range(outer_iters):
        if verbose and detector_enabled and not oblivion:
            print(f"{'-'*50}")
            print(f"Outer step: {outer_step + 1}, CONST: {torch.unique(CONST)}")

        # Initialize perturbation delta
        delta = torch.zeros_like(original_inputs, requires_grad=True, device=device)

        # Initialize dynamic variables
        prime_success = torch.zeros(batch_size, dtype=torch.bool, device=device)
        success_samples = torch.zeros(batch_size, dtype=torch.bool, device=device)

        # Inner optimization loop
        for step in range(max_iterations):
            adv_inputs = original_inputs + delta

            # Compute loss
            logits, prob = forward_model(adv_inputs, model, detector_enabled, oblivion)

            is_active = detector_enabled and not oblivion
            tau = model.tau[0].item() if is_active else 0.0

            loss = criterion(logits, targets)
            if is_active:
                loss = CONST * loss + (tau - prob)


            # Update success masks and distances
            with torch.no_grad():
                prime_success |= (logits.argmax(dim=1) == 0)
                success_mask = (logits.argmax(dim=1) == 0) & ((prob <= tau) if detector_enabled and not oblivion else True)
                success_samples |= success_mask
                o_success_samples |= success_mask

                dist = (adv_inputs != original_inputs).float().sum(dim=1)
                best_delta = torch.where(success_mask.unsqueeze(-1), delta.data.clone(), best_delta)


            if verbose and (step == 0 or ((step + 1) % (max_iterations // 10) == 0)):
                avg_dist = dist[o_success_samples].float().mean().item() if o_success_samples.any() else float('nan')
                print(
                    f"Iteration {step + 1:5} | "
                    f"LR = {step_length:.2f} | "
                    f"Current Success: {success_mask.sum().item():4}/{success_samples.sum().item():4} | "
                    f"Loop Success = {success_samples.sum().item():4}/{batch_size} | "
                    f"Total Success: {o_success_samples.sum().item():4}/{batch_size} | "
                    f"Best Dist (avg) = {avg_dist:3.2f} | "
                    f"Loss = {loss.mean().item():5.2f} | "
                )

            # Compute gradient
            grad_vars = torch.autograd.grad(loss.mean(), delta)
            gradients = grad_vars[0].data

            grad4insertion = (gradients >= 0) *(adv_inputs < 1.) * gradients
            grad4removal = (gradients < 0) * (adv_inputs > 0.) * modifiable_features_mask * gradients

            gradients = grad4removal + grad4insertion
            
            norm = norm.lower()

            # Norm
            if norm == 'linf':
                perturbation = torch.sign(gradients).float()

            elif norm == 'l2':
                #perturbation = (gradients / l2norm )
                l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)
                perturbation = (gradients / (l2norm + 1e-20)).float()

            elif norm == 'l1':
                # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)
                un_mod = torch.abs(delta.data) <= 1e-6
                gradients = gradients * un_mod
                max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]
                #print('max_grad ',max_grad)
                perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()

                if torch.all(success_mask):
                    break
                perturbation[success_mask] = 0.

            else:
                raise ValueError("Expect 'L1' or 'L2' or 'Linf' norm.")

            with torch.no_grad():
                # Update x_next
                delta.data = (delta.data + perturbation * step_length)

                delta.data.add_(original_inputs).clamp_(0, 1).sub_(original_inputs)


        # Update bounds and constants
        CONST, upper_bound, lower_bound = update_bounds_and_consts(
            outer_step, binary_search_steps, CONST, upper_bound, lower_bound, prime_success, update_factor = 100.
        )


    if (norm == 'l2' or norm == 'linf'):
        output = thresholded_binary_rounding((original_inputs + best_delta)).clamp_(0, 1)
    else:
        output = (original_inputs + best_delta).clamp_(0, 1)

    # ---- MODIFICATION: Restore the initially successful samples at the very end ----
    output[initially_successful_mask] = original_inputs[initially_successful_mask]
    
    return output
